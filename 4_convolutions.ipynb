{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Assignment 4\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_weights  [5, 5, 1, 16]\n",
      "layer2_weights  [5, 5, 16, 16]\n",
      "layer3_weights  784 64\n",
      "layer4_weights  [64, 10]\n",
      "shape of hidden1  [160, 14, 14, 16]\n",
      "shape of hidden2  [160, 7, 7, 16]\n",
      "shape of hidden3  [160, 64]\n",
      "shape of hidden1  [10000, 14, 14, 16]\n",
      "shape of hidden2  [10000, 7, 7, 16]\n",
      "shape of hidden3  [10000, 64]\n",
      "shape of hidden1  [10000, 14, 14, 16]\n",
      "shape of hidden2  [10000, 7, 7, 16]\n",
      "shape of hidden3  [10000, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 160\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  print(\"layer1_weights \", layer1_weights.get_shape().as_list())\n",
    "  # patch_size is filter matrix size\n",
    "  # depth means how many filters\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  print(\"layer2_weights \", layer2_weights.get_shape().as_list())\n",
    "\n",
    "  # layer3 is fully connected\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  print(\"layer3_weights \", image_size // 4 * image_size // 4 * depth, num_hidden)\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  print(\"layer4_weights \", layer4_weights.get_shape().as_list())\n",
    "\n",
    "# Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(input=data, filter=layer1_weights, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    print(\"shape of hidden1 \", hidden.get_shape().as_list())\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(\"shape of hidden2 \", shape)\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    print(\"shape of hidden3 \", hidden.get_shape().as_list())\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.486063\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.405953\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 53.7%\n",
      "Minibatch loss at step 100: 1.037153\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 150: 0.391102\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 200: 0.849277\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 250: 1.269853\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 300: 0.363787\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 350: 0.402243\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 400: 0.341402\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 450: 0.910895\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.771414\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 550: 0.945055\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 600: 0.251249\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 650: 0.800090\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 700: 0.818396\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 750: 0.067191\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 800: 0.555282\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 850: 0.914395\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 900: 0.556342\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 950: 0.477633\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1000: 0.341038\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden1 size  Tensor(\"Relu:0\", shape=(128, 28, 28, 16), dtype=float32)\n",
      "pool1 size  Tensor(\"MaxPool:0\", shape=(128, 14, 14, 16), dtype=float32)\n",
      "hidden2 size  Tensor(\"Relu_1:0\", shape=(128, 14, 14, 16), dtype=float32)\n",
      "pool2 size  Tensor(\"MaxPool_1:0\", shape=(128, 7, 7, 16), dtype=float32)\n",
      "hidden3 size  Tensor(\"Relu_2:0\", shape=(128, 64), dtype=float32)\n",
      "hidden1 size  Tensor(\"Relu_3:0\", shape=(10000, 28, 28, 16), dtype=float32)\n",
      "pool1 size  Tensor(\"MaxPool_2:0\", shape=(10000, 14, 14, 16), dtype=float32)\n",
      "hidden2 size  Tensor(\"Relu_4:0\", shape=(10000, 14, 14, 16), dtype=float32)\n",
      "pool2 size  Tensor(\"MaxPool_3:0\", shape=(10000, 7, 7, 16), dtype=float32)\n",
      "hidden3 size  Tensor(\"Relu_5:0\", shape=(10000, 64), dtype=float32)\n",
      "hidden1 size  Tensor(\"Relu_6:0\", shape=(10000, 28, 28, 16), dtype=float32)\n",
      "pool1 size  Tensor(\"MaxPool_4:0\", shape=(10000, 14, 14, 16), dtype=float32)\n",
      "hidden2 size  Tensor(\"Relu_7:0\", shape=(10000, 14, 14, 16), dtype=float32)\n",
      "pool2 size  Tensor(\"MaxPool_5:0\", shape=(10000, 7, 7, 16), dtype=float32)\n",
      "hidden3 size  Tensor(\"Relu_8:0\", shape=(10000, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model_pool(data, train=False):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        print(\"hidden1 size \", hidden)\n",
    "        pool = tf.nn.max_pool(value=hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        print(\"pool1 size \", pool)\n",
    "    \n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        print(\"hidden2 size \", hidden)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        print(\"pool2 size \", pool)\n",
    "    \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        print(\"hidden3 size \", hidden)\n",
    "    \n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model_pool(tf_train_dataset, True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "   \n",
    "    # adding regularizers\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases)\n",
    "                   )\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 3e-4 * regularizers\n",
    "  \n",
    "    # Optimizer: set up a variable that's incremented once per batch and controls the learning rate decay.\n",
    "    batch = tf.Variable(0)\n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * batch_size,  # Current index into the dataset.\n",
    "        train_labels.shape[0],          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss,global_step=batch)\n",
    "   \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model_pool(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model_pool(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 4.66382\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 50 : 2.07626\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 51.6%\n",
      "Minibatch loss at step 100 : 1.47292\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 150 : 1.39927\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 200 : 1.12975\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 250 : 0.89598\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 300 : 0.967837\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 350 : 0.828221\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 400 : 0.935566\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 450 : 0.751001\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 500 : 0.684769\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 550 : 1.02332\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 600 : 0.906778\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 650 : 0.87269\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 700 : 0.791746\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 750 : 0.677326\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 800 : 0.660108\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 850 : 0.683646\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 900 : 0.713673\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 950 : 0.722425\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000 : 0.742341\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1050 : 0.604019\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1100 : 0.754334\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1150 : 0.827499\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1200 : 0.557116\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1250 : 0.591488\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1300 : 0.744757\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1350 : 0.737099\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1400 : 0.505101\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1450 : 0.59651\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500 : 0.460512\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1550 : 0.717931\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1600 : 0.792672\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1650 : 0.634149\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1700 : 0.625946\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1750 : 0.5643\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1800 : 0.599669\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1850 : 0.727192\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1900 : 0.758556\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1950 : 0.476247\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2000 : 0.528138\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2050 : 0.527967\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2100 : 0.537952\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2150 : 0.638423\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2200 : 0.691249\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2250 : 0.792049\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2300 : 0.624905\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2350 : 0.529939\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2400 : 0.569014\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2450 : 0.585424\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2500 : 0.519707\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2550 : 0.475651\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2600 : 0.638087\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2650 : 0.502197\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2700 : 0.651591\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2750 : 0.3922\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2800 : 0.522621\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2850 : 0.505051\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2900 : 0.398932\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2950 : 0.501104\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3000 : 0.648511\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in xrange(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print(\"Minibatch loss at step\", step, \":\", l)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "batch_size=64\n",
    "PIXEL_DEPTH = 255\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-2ef0afbef12b>:142: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized!\n",
      "Step 0 (epoch 0.00), 4.7 ms\n",
      "Minibatch loss: 9.655, learning rate: 0.010000\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 18.7%\n",
      "Step 100 (epoch 0.03), 171.0 ms\n",
      "Minibatch loss: 3.711, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.1%\n",
      "Step 200 (epoch 0.06), 162.5 ms\n",
      "Minibatch loss: 3.632, learning rate: 0.010000\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.2%\n",
      "Step 300 (epoch 0.10), 158.6 ms\n",
      "Minibatch loss: 3.824, learning rate: 0.010000\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 84.2%\n",
      "Step 400 (epoch 0.13), 158.3 ms\n",
      "Minibatch loss: 3.956, learning rate: 0.010000\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.9%\n",
      "Step 500 (epoch 0.16), 163.0 ms\n",
      "Minibatch loss: 3.416, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.5%\n",
      "Step 600 (epoch 0.19), 158.8 ms\n",
      "Minibatch loss: 3.447, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Step 700 (epoch 0.22), 160.6 ms\n",
      "Minibatch loss: 3.283, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.5%\n",
      "Step 800 (epoch 0.26), 157.6 ms\n",
      "Minibatch loss: 3.366, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.5%\n",
      "Step 900 (epoch 0.29), 163.3 ms\n",
      "Minibatch loss: 3.430, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.9%\n",
      "Step 1000 (epoch 0.32), 159.9 ms\n",
      "Minibatch loss: 3.203, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Step 1100 (epoch 0.35), 167.4 ms\n",
      "Minibatch loss: 3.538, learning rate: 0.010000\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.6%\n",
      "Step 1200 (epoch 0.38), 161.6 ms\n",
      "Minibatch loss: 3.085, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.4%\n",
      "Step 1300 (epoch 0.42), 160.7 ms\n",
      "Minibatch loss: 3.002, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.5%\n",
      "Step 1400 (epoch 0.45), 160.5 ms\n",
      "Minibatch loss: 3.257, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Step 1500 (epoch 0.48), 156.4 ms\n",
      "Minibatch loss: 3.148, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1600 (epoch 0.51), 158.8 ms\n",
      "Minibatch loss: 2.916, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Step 1700 (epoch 0.54), 157.1 ms\n",
      "Minibatch loss: 3.100, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.2%\n",
      "Step 1800 (epoch 0.58), 159.5 ms\n",
      "Minibatch loss: 2.935, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Step 1900 (epoch 0.61), 161.5 ms\n",
      "Minibatch loss: 2.967, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Step 2000 (epoch 0.64), 156.4 ms\n",
      "Minibatch loss: 2.909, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Step 2100 (epoch 0.67), 160.7 ms\n",
      "Minibatch loss: 2.752, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Step 2200 (epoch 0.70), 159.5 ms\n",
      "Minibatch loss: 2.897, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.4%\n",
      "Step 2300 (epoch 0.74), 159.9 ms\n",
      "Minibatch loss: 3.155, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.6%\n",
      "Step 2400 (epoch 0.77), 159.0 ms\n",
      "Minibatch loss: 2.825, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Step 2500 (epoch 0.80), 162.2 ms\n",
      "Minibatch loss: 2.745, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2600 (epoch 0.83), 157.2 ms\n",
      "Minibatch loss: 2.628, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Step 2700 (epoch 0.86), 158.6 ms\n",
      "Minibatch loss: 3.012, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Step 2800 (epoch 0.90), 158.2 ms\n",
      "Minibatch loss: 2.756, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2900 (epoch 0.93), 158.8 ms\n",
      "Minibatch loss: 2.853, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Step 3000 (epoch 0.96), 158.0 ms\n",
      "Minibatch loss: 2.686, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3100 (epoch 0.99), 165.7 ms\n",
      "Minibatch loss: 2.830, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3200 (epoch 1.02), 158.6 ms\n",
      "Minibatch loss: 2.620, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3300 (epoch 1.06), 157.2 ms\n",
      "Minibatch loss: 2.610, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3400 (epoch 1.09), 156.4 ms\n",
      "Minibatch loss: 2.489, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.5%\n",
      "Step 3500 (epoch 1.12), 158.1 ms\n",
      "Minibatch loss: 2.491, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Step 3600 (epoch 1.15), 158.0 ms\n",
      "Minibatch loss: 2.669, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3700 (epoch 1.18), 157.7 ms\n",
      "Minibatch loss: 2.396, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3800 (epoch 1.22), 169.8 ms\n",
      "Minibatch loss: 2.755, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3900 (epoch 1.25), 158.9 ms\n",
      "Minibatch loss: 2.350, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4000 (epoch 1.28), 157.3 ms\n",
      "Minibatch loss: 2.486, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4100 (epoch 1.31), 158.2 ms\n",
      "Minibatch loss: 2.534, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4200 (epoch 1.34), 158.4 ms\n",
      "Minibatch loss: 2.651, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.4%\n",
      "Step 4300 (epoch 1.38), 158.1 ms\n",
      "Minibatch loss: 2.447, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Step 4400 (epoch 1.41), 160.9 ms\n",
      "Minibatch loss: 2.271, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4500 (epoch 1.44), 158.9 ms\n",
      "Minibatch loss: 2.173, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4600 (epoch 1.47), 162.4 ms\n",
      "Minibatch loss: 2.545, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.5%\n",
      "Step 4700 (epoch 1.50), 159.3 ms\n",
      "Minibatch loss: 2.449, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.5%\n",
      "Step 4800 (epoch 1.54), 158.1 ms\n",
      "Minibatch loss: 2.471, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4900 (epoch 1.57), 158.8 ms\n",
      "Minibatch loss: 2.149, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Step 5000 (epoch 1.60), 159.1 ms\n",
      "Minibatch loss: 2.276, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5100 (epoch 1.63), 162.7 ms\n",
      "Minibatch loss: 2.488, learning rate: 0.009500\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5200 (epoch 1.66), 157.7 ms\n",
      "Minibatch loss: 2.387, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5300 (epoch 1.70), 157.3 ms\n",
      "Minibatch loss: 2.366, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5400 (epoch 1.73), 160.4 ms\n",
      "Minibatch loss: 2.277, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5500 (epoch 1.76), 157.6 ms\n",
      "Minibatch loss: 2.196, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5600 (epoch 1.79), 161.5 ms\n",
      "Minibatch loss: 2.084, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5700 (epoch 1.82), 157.9 ms\n",
      "Minibatch loss: 2.128, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Step 5800 (epoch 1.86), 157.9 ms\n",
      "Minibatch loss: 2.258, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5900 (epoch 1.89), 158.5 ms\n",
      "Minibatch loss: 2.164, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 90.0%\n",
      "Step 6000 (epoch 1.92), 158.4 ms\n",
      "Minibatch loss: 1.878, learning rate: 0.009500\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6100 (epoch 1.95), 155.8 ms\n",
      "Minibatch loss: 2.044, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Step 6200 (epoch 1.98), 159.7 ms\n",
      "Minibatch loss: 2.096, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Step 6300 (epoch 2.02), 158.7 ms\n",
      "Minibatch loss: 2.147, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6400 (epoch 2.05), 152.4 ms\n",
      "Minibatch loss: 2.030, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6500 (epoch 2.08), 156.9 ms\n",
      "Minibatch loss: 1.843, learning rate: 0.009025\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6600 (epoch 2.11), 161.3 ms\n",
      "Minibatch loss: 1.911, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6700 (epoch 2.14), 158.4 ms\n",
      "Minibatch loss: 1.949, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6800 (epoch 2.18), 160.1 ms\n",
      "Minibatch loss: 1.767, learning rate: 0.009025\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.9%\n",
      "Step 6900 (epoch 2.21), 159.5 ms\n",
      "Minibatch loss: 1.898, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7000 (epoch 2.24), 159.6 ms\n",
      "Minibatch loss: 1.872, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7100 (epoch 2.27), 159.7 ms\n",
      "Minibatch loss: 1.866, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Step 7200 (epoch 2.30), 155.9 ms\n",
      "Minibatch loss: 1.969, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7300 (epoch 2.34), 156.5 ms\n",
      "Minibatch loss: 1.941, learning rate: 0.009025\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7400 (epoch 2.37), 161.7 ms\n",
      "Minibatch loss: 2.122, learning rate: 0.009025\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 90.2%\n",
      "Step 7500 (epoch 2.40), 160.3 ms\n",
      "Minibatch loss: 1.903, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7600 (epoch 2.43), 155.8 ms\n",
      "Minibatch loss: 1.910, learning rate: 0.009025\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7700 (epoch 2.46), 161.3 ms\n",
      "Minibatch loss: 1.757, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7800 (epoch 2.50), 156.4 ms\n",
      "Minibatch loss: 1.820, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7900 (epoch 2.53), 167.1 ms\n",
      "Minibatch loss: 1.762, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Step 8000 (epoch 2.56), 173.2 ms\n",
      "Minibatch loss: 1.799, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Step 8100 (epoch 2.59), 175.2 ms\n",
      "Minibatch loss: 1.723, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8200 (epoch 2.62), 174.6 ms\n",
      "Minibatch loss: 1.689, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8300 (epoch 2.66), 171.0 ms\n",
      "Minibatch loss: 1.623, learning rate: 0.009025\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8400 (epoch 2.69), 176.3 ms\n",
      "Minibatch loss: 1.931, learning rate: 0.009025\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8500 (epoch 2.72), 173.4 ms\n",
      "Minibatch loss: 1.864, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8600 (epoch 2.75), 164.3 ms\n",
      "Minibatch loss: 1.685, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8700 (epoch 2.78), 157.7 ms\n",
      "Minibatch loss: 1.849, learning rate: 0.009025\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8800 (epoch 2.82), 157.6 ms\n",
      "Minibatch loss: 1.706, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Step 8900 (epoch 2.85), 160.9 ms\n",
      "Minibatch loss: 1.652, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 9000 (epoch 2.88), 160.1 ms\n",
      "Minibatch loss: 1.514, learning rate: 0.009025\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9100 (epoch 2.91), 159.0 ms\n",
      "Minibatch loss: 1.661, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9200 (epoch 2.94), 157.0 ms\n",
      "Minibatch loss: 1.600, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9300 (epoch 2.98), 162.7 ms\n",
      "Minibatch loss: 1.600, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 9400 (epoch 3.01), 154.7 ms\n",
      "Minibatch loss: 1.584, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 9500 (epoch 3.04), 158.8 ms\n",
      "Minibatch loss: 1.577, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Step 9600 (epoch 3.07), 159.2 ms\n",
      "Minibatch loss: 1.556, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Step 9700 (epoch 3.10), 157.9 ms\n",
      "Minibatch loss: 1.446, learning rate: 0.008574\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.9%\n",
      "Step 9800 (epoch 3.14), 158.0 ms\n",
      "Minibatch loss: 1.512, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9900 (epoch 3.17), 157.2 ms\n",
      "Minibatch loss: 1.587, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Step 10000 (epoch 3.20), 157.9 ms\n",
      "Minibatch loss: 1.397, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10100 (epoch 3.23), 159.2 ms\n",
      "Minibatch loss: 1.546, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10200 (epoch 3.26), 157.1 ms\n",
      "Minibatch loss: 1.365, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10300 (epoch 3.30), 157.7 ms\n",
      "Minibatch loss: 1.532, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Step 10400 (epoch 3.33), 156.7 ms\n",
      "Minibatch loss: 1.537, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10500 (epoch 3.36), 159.0 ms\n",
      "Minibatch loss: 1.555, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10600 (epoch 3.39), 154.8 ms\n",
      "Minibatch loss: 1.315, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10700 (epoch 3.42), 157.3 ms\n",
      "Minibatch loss: 1.398, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10800 (epoch 3.46), 159.6 ms\n",
      "Minibatch loss: 1.741, learning rate: 0.008574\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10900 (epoch 3.49), 157.8 ms\n",
      "Minibatch loss: 1.498, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11000 (epoch 3.52), 157.8 ms\n",
      "Minibatch loss: 1.364, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Step 11100 (epoch 3.55), 156.4 ms\n",
      "Minibatch loss: 1.333, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11200 (epoch 3.58), 156.6 ms\n",
      "Minibatch loss: 1.394, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11300 (epoch 3.62), 157.9 ms\n",
      "Minibatch loss: 1.331, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11400 (epoch 3.65), 157.8 ms\n",
      "Minibatch loss: 1.267, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11500 (epoch 3.68), 154.9 ms\n",
      "Minibatch loss: 1.389, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11600 (epoch 3.71), 157.2 ms\n",
      "Minibatch loss: 1.519, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11700 (epoch 3.74), 157.9 ms\n",
      "Minibatch loss: 1.342, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11800 (epoch 3.78), 154.5 ms\n",
      "Minibatch loss: 1.364, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11900 (epoch 3.81), 155.5 ms\n",
      "Minibatch loss: 1.278, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Step 12000 (epoch 3.84), 156.8 ms\n",
      "Minibatch loss: 1.299, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12100 (epoch 3.87), 156.4 ms\n",
      "Minibatch loss: 1.231, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 91.3%\n",
      "Step 12200 (epoch 3.90), 158.2 ms\n",
      "Minibatch loss: 1.420, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12300 (epoch 3.94), 158.5 ms\n",
      "Minibatch loss: 1.225, learning rate: 0.008574\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12400 (epoch 3.97), 158.0 ms\n",
      "Minibatch loss: 1.337, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12500 (epoch 4.00), 157.6 ms\n",
      "Minibatch loss: 1.136, learning rate: 0.008145\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12600 (epoch 4.03), 156.4 ms\n",
      "Minibatch loss: 1.306, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12700 (epoch 4.06), 157.7 ms\n",
      "Minibatch loss: 1.199, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12800 (epoch 4.10), 152.9 ms\n",
      "Minibatch loss: 1.359, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12900 (epoch 4.13), 157.5 ms\n",
      "Minibatch loss: 1.030, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13000 (epoch 4.16), 156.0 ms\n",
      "Minibatch loss: 1.387, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.1%\n",
      "Step 13100 (epoch 4.19), 153.6 ms\n",
      "Minibatch loss: 1.274, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13200 (epoch 4.22), 157.8 ms\n",
      "Minibatch loss: 1.304, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 13300 (epoch 4.26), 155.9 ms\n",
      "Minibatch loss: 1.107, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13400 (epoch 4.29), 152.9 ms\n",
      "Minibatch loss: 1.012, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13500 (epoch 4.32), 156.3 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13600 (epoch 4.35), 157.8 ms\n",
      "Minibatch loss: 1.253, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13700 (epoch 4.38), 157.5 ms\n",
      "Minibatch loss: 1.267, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13800 (epoch 4.42), 157.9 ms\n",
      "Minibatch loss: 1.139, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13900 (epoch 4.45), 156.7 ms\n",
      "Minibatch loss: 1.187, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14000 (epoch 4.48), 158.0 ms\n",
      "Minibatch loss: 1.015, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Step 14100 (epoch 4.51), 158.8 ms\n",
      "Minibatch loss: 1.058, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Step 14200 (epoch 4.54), 157.3 ms\n",
      "Minibatch loss: 1.173, learning rate: 0.008145\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.3%\n",
      "Step 14300 (epoch 4.58), 158.8 ms\n",
      "Minibatch loss: 1.170, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14400 (epoch 4.61), 155.4 ms\n",
      "Minibatch loss: 1.004, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14500 (epoch 4.64), 158.8 ms\n",
      "Minibatch loss: 1.086, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 14600 (epoch 4.67), 157.8 ms\n",
      "Minibatch loss: 1.229, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14700 (epoch 4.70), 158.4 ms\n",
      "Minibatch loss: 1.001, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14800 (epoch 4.74), 157.9 ms\n",
      "Minibatch loss: 1.271, learning rate: 0.008145\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14900 (epoch 4.77), 160.3 ms\n",
      "Minibatch loss: 1.080, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15000 (epoch 4.80), 177.6 ms\n",
      "Minibatch loss: 1.042, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15100 (epoch 4.83), 161.7 ms\n",
      "Minibatch loss: 1.006, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15200 (epoch 4.86), 158.9 ms\n",
      "Minibatch loss: 1.112, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15300 (epoch 4.90), 159.5 ms\n",
      "Minibatch loss: 1.178, learning rate: 0.008145\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15400 (epoch 4.93), 158.7 ms\n",
      "Minibatch loss: 1.056, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15500 (epoch 4.96), 158.9 ms\n",
      "Minibatch loss: 1.008, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15600 (epoch 4.99), 159.9 ms\n",
      "Minibatch loss: 1.070, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15700 (epoch 5.02), 155.5 ms\n",
      "Minibatch loss: 1.127, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15800 (epoch 5.06), 159.0 ms\n",
      "Minibatch loss: 1.072, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15900 (epoch 5.09), 158.7 ms\n",
      "Minibatch loss: 0.925, learning rate: 0.007738\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16000 (epoch 5.12), 158.4 ms\n",
      "Minibatch loss: 1.055, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16100 (epoch 5.15), 157.5 ms\n",
      "Minibatch loss: 1.076, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 16200 (epoch 5.18), 157.8 ms\n",
      "Minibatch loss: 0.885, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 16300 (epoch 5.22), 159.1 ms\n",
      "Minibatch loss: 1.023, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16400 (epoch 5.25), 154.6 ms\n",
      "Minibatch loss: 0.986, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16500 (epoch 5.28), 156.4 ms\n",
      "Minibatch loss: 0.981, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16600 (epoch 5.31), 157.0 ms\n",
      "Minibatch loss: 1.059, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16700 (epoch 5.34), 156.8 ms\n",
      "Minibatch loss: 1.076, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16800 (epoch 5.38), 156.2 ms\n",
      "Minibatch loss: 1.043, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16900 (epoch 5.41), 156.5 ms\n",
      "Minibatch loss: 1.014, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17000 (epoch 5.44), 157.6 ms\n",
      "Minibatch loss: 1.024, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17100 (epoch 5.47), 155.7 ms\n",
      "Minibatch loss: 1.017, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 17200 (epoch 5.50), 157.1 ms\n",
      "Minibatch loss: 0.895, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17300 (epoch 5.54), 157.2 ms\n",
      "Minibatch loss: 1.059, learning rate: 0.007738\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.4%\n",
      "Step 17400 (epoch 5.57), 157.3 ms\n",
      "Minibatch loss: 0.978, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17500 (epoch 5.60), 156.9 ms\n",
      "Minibatch loss: 0.872, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17600 (epoch 5.63), 157.3 ms\n",
      "Minibatch loss: 0.930, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17700 (epoch 5.66), 156.7 ms\n",
      "Minibatch loss: 0.870, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17800 (epoch 5.70), 157.6 ms\n",
      "Minibatch loss: 1.023, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17900 (epoch 5.73), 157.7 ms\n",
      "Minibatch loss: 0.880, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18000 (epoch 5.76), 156.0 ms\n",
      "Minibatch loss: 0.925, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18100 (epoch 5.79), 157.9 ms\n",
      "Minibatch loss: 0.853, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Step 18200 (epoch 5.82), 156.6 ms\n",
      "Minibatch loss: 0.846, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18300 (epoch 5.86), 157.7 ms\n",
      "Minibatch loss: 0.845, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18400 (epoch 5.89), 155.0 ms\n",
      "Minibatch loss: 0.926, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18500 (epoch 5.92), 156.3 ms\n",
      "Minibatch loss: 0.812, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 18600 (epoch 5.95), 156.6 ms\n",
      "Minibatch loss: 0.837, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18700 (epoch 5.98), 156.4 ms\n",
      "Minibatch loss: 0.755, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18800 (epoch 6.02), 155.8 ms\n",
      "Minibatch loss: 0.884, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 18900 (epoch 6.05), 152.4 ms\n",
      "Minibatch loss: 0.805, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Step 19000 (epoch 6.08), 156.1 ms\n",
      "Minibatch loss: 0.739, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Step 19100 (epoch 6.11), 158.2 ms\n",
      "Minibatch loss: 0.950, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19200 (epoch 6.14), 153.3 ms\n",
      "Minibatch loss: 0.737, learning rate: 0.007351\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.6%\n",
      "Step 19300 (epoch 6.18), 157.5 ms\n",
      "Minibatch loss: 0.837, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19400 (epoch 6.21), 155.8 ms\n",
      "Minibatch loss: 0.730, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19500 (epoch 6.24), 157.6 ms\n",
      "Minibatch loss: 0.796, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19600 (epoch 6.27), 154.0 ms\n",
      "Minibatch loss: 0.763, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19700 (epoch 6.30), 154.5 ms\n",
      "Minibatch loss: 0.925, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19800 (epoch 6.34), 156.7 ms\n",
      "Minibatch loss: 0.735, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19900 (epoch 6.37), 158.2 ms\n",
      "Minibatch loss: 0.758, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 20000 (epoch 6.40), 155.2 ms\n",
      "Minibatch loss: 0.738, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Step 20100 (epoch 6.43), 155.5 ms\n",
      "Minibatch loss: 1.059, learning rate: 0.007351\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20200 (epoch 6.46), 155.2 ms\n",
      "Minibatch loss: 0.897, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20300 (epoch 6.50), 155.7 ms\n",
      "Minibatch loss: 0.887, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 20400 (epoch 6.53), 157.1 ms\n",
      "Minibatch loss: 0.961, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Step 20500 (epoch 6.56), 154.8 ms\n",
      "Minibatch loss: 0.709, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20600 (epoch 6.59), 155.7 ms\n",
      "Minibatch loss: 0.728, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20700 (epoch 6.62), 154.9 ms\n",
      "Minibatch loss: 0.635, learning rate: 0.007351\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20800 (epoch 6.66), 156.1 ms\n",
      "Minibatch loss: 0.864, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20900 (epoch 6.69), 156.4 ms\n",
      "Minibatch loss: 0.840, learning rate: 0.007351\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 21000 (epoch 6.72), 157.4 ms\n",
      "Minibatch loss: 0.890, learning rate: 0.007351\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Step 21100 (epoch 6.75), 155.1 ms\n",
      "Minibatch loss: 0.819, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 21200 (epoch 6.78), 151.8 ms\n",
      "Minibatch loss: 0.678, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21300 (epoch 6.82), 156.6 ms\n",
      "Minibatch loss: 0.692, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21400 (epoch 6.85), 154.7 ms\n",
      "Minibatch loss: 0.727, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21500 (epoch 6.88), 156.3 ms\n",
      "Minibatch loss: 0.774, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21600 (epoch 6.91), 154.3 ms\n",
      "Minibatch loss: 0.697, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 21700 (epoch 6.94), 155.0 ms\n",
      "Minibatch loss: 0.681, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21800 (epoch 6.98), 155.7 ms\n",
      "Minibatch loss: 0.759, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21900 (epoch 7.01), 157.1 ms\n",
      "Minibatch loss: 0.724, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 22000 (epoch 7.04), 152.0 ms\n",
      "Minibatch loss: 0.619, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 22100 (epoch 7.07), 156.8 ms\n",
      "Minibatch loss: 0.768, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22200 (epoch 7.10), 154.7 ms\n",
      "Minibatch loss: 0.892, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22300 (epoch 7.14), 158.5 ms\n",
      "Minibatch loss: 0.625, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22400 (epoch 7.17), 156.9 ms\n",
      "Minibatch loss: 0.902, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 22500 (epoch 7.20), 158.2 ms\n",
      "Minibatch loss: 0.698, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 22600 (epoch 7.23), 152.6 ms\n",
      "Minibatch loss: 0.827, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22700 (epoch 7.26), 155.7 ms\n",
      "Minibatch loss: 0.804, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22800 (epoch 7.30), 157.4 ms\n",
      "Minibatch loss: 0.693, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22900 (epoch 7.33), 156.7 ms\n",
      "Minibatch loss: 0.733, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23000 (epoch 7.36), 155.5 ms\n",
      "Minibatch loss: 0.794, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23100 (epoch 7.39), 152.8 ms\n",
      "Minibatch loss: 0.575, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23200 (epoch 7.42), 157.0 ms\n",
      "Minibatch loss: 1.238, learning rate: 0.006983\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23300 (epoch 7.46), 156.2 ms\n",
      "Minibatch loss: 0.765, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23400 (epoch 7.49), 155.4 ms\n",
      "Minibatch loss: 0.686, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23500 (epoch 7.52), 154.1 ms\n",
      "Minibatch loss: 0.680, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23600 (epoch 7.55), 157.1 ms\n",
      "Minibatch loss: 0.569, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23700 (epoch 7.58), 155.6 ms\n",
      "Minibatch loss: 0.550, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23800 (epoch 7.62), 153.5 ms\n",
      "Minibatch loss: 0.642, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23900 (epoch 7.65), 155.9 ms\n",
      "Minibatch loss: 0.675, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24000 (epoch 7.68), 156.1 ms\n",
      "Minibatch loss: 0.790, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 24100 (epoch 7.71), 155.8 ms\n",
      "Minibatch loss: 0.579, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24200 (epoch 7.74), 156.1 ms\n",
      "Minibatch loss: 0.665, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24300 (epoch 7.78), 155.4 ms\n",
      "Minibatch loss: 0.747, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24400 (epoch 7.81), 156.7 ms\n",
      "Minibatch loss: 0.607, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 92.2%\n",
      "Step 24500 (epoch 7.84), 155.8 ms\n",
      "Minibatch loss: 0.893, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.1%\n",
      "Step 24600 (epoch 7.87), 156.6 ms\n",
      "Minibatch loss: 0.644, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24700 (epoch 7.90), 156.3 ms\n",
      "Minibatch loss: 0.561, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24800 (epoch 7.94), 157.7 ms\n",
      "Minibatch loss: 0.685, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24900 (epoch 7.97), 155.4 ms\n",
      "Minibatch loss: 0.629, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25000 (epoch 8.00), 155.5 ms\n",
      "Minibatch loss: 0.711, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25100 (epoch 8.03), 152.7 ms\n",
      "Minibatch loss: 0.744, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25200 (epoch 8.06), 157.0 ms\n",
      "Minibatch loss: 0.558, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25300 (epoch 8.10), 155.5 ms\n",
      "Minibatch loss: 0.614, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25400 (epoch 8.13), 154.9 ms\n",
      "Minibatch loss: 0.857, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25500 (epoch 8.16), 156.3 ms\n",
      "Minibatch loss: 0.601, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 25600 (epoch 8.19), 153.3 ms\n",
      "Minibatch loss: 0.599, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 25700 (epoch 8.22), 156.2 ms\n",
      "Minibatch loss: 0.573, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 25800 (epoch 8.26), 157.0 ms\n",
      "Minibatch loss: 0.615, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25900 (epoch 8.29), 154.5 ms\n",
      "Minibatch loss: 0.696, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26000 (epoch 8.32), 154.2 ms\n",
      "Minibatch loss: 0.576, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 26100 (epoch 8.35), 156.6 ms\n",
      "Minibatch loss: 0.553, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 26200 (epoch 8.38), 156.5 ms\n",
      "Minibatch loss: 0.618, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26300 (epoch 8.42), 153.8 ms\n",
      "Minibatch loss: 0.637, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 26400 (epoch 8.45), 155.6 ms\n",
      "Minibatch loss: 0.675, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 26500 (epoch 8.48), 153.9 ms\n",
      "Minibatch loss: 0.700, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 26600 (epoch 8.51), 155.8 ms\n",
      "Minibatch loss: 0.541, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 26700 (epoch 8.54), 156.8 ms\n",
      "Minibatch loss: 0.552, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 26800 (epoch 8.58), 154.2 ms\n",
      "Minibatch loss: 0.740, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Step 26900 (epoch 8.61), 156.1 ms\n",
      "Minibatch loss: 0.670, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.1%\n",
      "Step 27000 (epoch 8.64), 154.5 ms\n",
      "Minibatch loss: 0.720, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27100 (epoch 8.67), 153.8 ms\n",
      "Minibatch loss: 0.557, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27200 (epoch 8.70), 153.2 ms\n",
      "Minibatch loss: 0.623, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 27300 (epoch 8.74), 154.6 ms\n",
      "Minibatch loss: 0.462, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27400 (epoch 8.77), 155.0 ms\n",
      "Minibatch loss: 0.638, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27500 (epoch 8.80), 152.9 ms\n",
      "Minibatch loss: 0.715, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27600 (epoch 8.83), 155.3 ms\n",
      "Minibatch loss: 0.449, learning rate: 0.006634\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27700 (epoch 8.86), 155.7 ms\n",
      "Minibatch loss: 0.582, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27800 (epoch 8.90), 154.7 ms\n",
      "Minibatch loss: 0.549, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27900 (epoch 8.93), 152.9 ms\n",
      "Minibatch loss: 0.650, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28000 (epoch 8.96), 157.5 ms\n",
      "Minibatch loss: 0.622, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28100 (epoch 8.99), 157.4 ms\n",
      "Minibatch loss: 0.637, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28200 (epoch 9.02), 154.3 ms\n",
      "Minibatch loss: 0.635, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28300 (epoch 9.06), 157.3 ms\n",
      "Minibatch loss: 0.543, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28400 (epoch 9.09), 156.4 ms\n",
      "Minibatch loss: 0.520, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28500 (epoch 9.12), 158.2 ms\n",
      "Minibatch loss: 0.552, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28600 (epoch 9.15), 156.6 ms\n",
      "Minibatch loss: 0.569, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28700 (epoch 9.18), 155.7 ms\n",
      "Minibatch loss: 0.585, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 28800 (epoch 9.22), 154.4 ms\n",
      "Minibatch loss: 0.468, learning rate: 0.006302\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28900 (epoch 9.25), 157.0 ms\n",
      "Minibatch loss: 0.544, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29000 (epoch 9.28), 156.1 ms\n",
      "Minibatch loss: 0.436, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29100 (epoch 9.31), 154.8 ms\n",
      "Minibatch loss: 0.530, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29200 (epoch 9.34), 157.2 ms\n",
      "Minibatch loss: 0.471, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 29300 (epoch 9.38), 156.8 ms\n",
      "Minibatch loss: 0.431, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 29400 (epoch 9.41), 154.6 ms\n",
      "Minibatch loss: 0.418, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 29500 (epoch 9.44), 153.2 ms\n",
      "Minibatch loss: 0.558, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29600 (epoch 9.47), 157.3 ms\n",
      "Minibatch loss: 0.563, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 29700 (epoch 9.50), 156.7 ms\n",
      "Minibatch loss: 0.696, learning rate: 0.006302\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 29800 (epoch 9.54), 156.1 ms\n",
      "Minibatch loss: 0.510, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29900 (epoch 9.57), 155.2 ms\n",
      "Minibatch loss: 0.562, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30000 (epoch 9.60), 157.5 ms\n",
      "Minibatch loss: 0.552, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30100 (epoch 9.63), 154.2 ms\n",
      "Minibatch loss: 0.493, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30200 (epoch 9.66), 157.5 ms\n",
      "Minibatch loss: 0.532, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30300 (epoch 9.70), 157.3 ms\n",
      "Minibatch loss: 0.436, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30400 (epoch 9.73), 157.7 ms\n",
      "Minibatch loss: 0.598, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30500 (epoch 9.76), 157.2 ms\n",
      "Minibatch loss: 0.703, learning rate: 0.006302\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30600 (epoch 9.79), 156.9 ms\n",
      "Minibatch loss: 0.496, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30700 (epoch 9.82), 158.5 ms\n",
      "Minibatch loss: 0.580, learning rate: 0.006302\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30800 (epoch 9.86), 156.5 ms\n",
      "Minibatch loss: 0.460, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30900 (epoch 9.89), 157.3 ms\n",
      "Minibatch loss: 0.517, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31000 (epoch 9.92), 157.9 ms\n",
      "Minibatch loss: 0.556, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31100 (epoch 9.95), 165.5 ms\n",
      "Minibatch loss: 0.592, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.6%\n",
      "Step 31200 (epoch 9.98), 173.1 ms\n",
      "Minibatch loss: 0.462, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Test accuracy: 97.3%\n"
     ]
    }
   ],
   "source": [
    "def lenet(argv=None):  # pylint: disable=unused-argument\n",
    "  train_size = train_labels.shape[0]\n",
    "\n",
    "  # This is where training samples and labels are fed to the graph.\n",
    "  # These placeholder nodes will be fed a batch of training data at each\n",
    "  # training step using the {feed_dict} argument to the Run() call below.\n",
    "  train_data_node = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(batch_size, image_size, image_size, num_channels))\n",
    "  train_labels_node = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_labels))\n",
    "  eval_data = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(EVAL_BATCH_SIZE, image_size, image_size, num_channels))\n",
    "\n",
    "  # The variables below hold all the trainable weights. They are passed an\n",
    "  # initial value which will be assigned when when we call:\n",
    "  # {tf.initialize_all_variables().run()}\n",
    "  conv1_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, num_channels, 32],  # 5x5 filter, depth 32.\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "  conv2_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, 32, 64],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "      tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * 64, 512],\n",
    "          stddev=0.1,\n",
    "          seed=SEED))\n",
    "  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "  fc2_weights = tf.Variable(\n",
    "      tf.truncated_normal([512, num_labels],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "\n",
    "  # We will replicate the model structure for the training subgraph, as well\n",
    "  # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "  def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "  # Training computation: logits + cross-entropy loss.\n",
    "  logits = model(train_data_node, True)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=train_labels_node))\n",
    "\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "\n",
    "  # Optimizer: set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  batch = tf.Variable(0)\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * batch_size,  # Current index into the dataset.\n",
    "      train_size,          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                         0.9).minimize(loss,\n",
    "                                                       global_step=batch)\n",
    "\n",
    "  # Predictions for the current training minibatch.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Predictions for the test and validation, which we'll compute less often.\n",
    "  eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "  # {eval_data} and pulling the results from {eval_predictions}.\n",
    "  # Saves memory and enables this to run on smaller GPUs.\n",
    "  def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, num_labels), dtype=np.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "  # Create a local session to run the training.\n",
    "  start_time = time.time()\n",
    "  with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    for step in xrange(int(NUM_EPOCHS * train_size) // batch_size):\n",
    "      # Compute the offset of the current minibatch in the data.\n",
    "      # Note that we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_size - batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), ...]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "      # This dictionary maps the batch data (as a numpy array) to the\n",
    "      # node in the graph is should be fed to.\n",
    "      feed_dict = {train_data_node: batch_data,\n",
    "                   train_labels_node: batch_labels}\n",
    "      # Run the graph and fetch some of the nodes.\n",
    "      _, l, lr, predictions = sess.run(\n",
    "          [optimizer, loss, learning_rate, train_prediction],\n",
    "          feed_dict=feed_dict)\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "              (step, float(step) * batch_size / train_size,\n",
    "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            eval_in_batches(valid_dataset, sess), valid_labels))\n",
    "        sys.stdout.flush()\n",
    "    # Finally print the result!\n",
    "    test_error = accuracy(eval_in_batches(test_dataset, sess), test_labels)\n",
    "    print('Test accuracy: %.1f%%' % test_error)\n",
    "    \n",
    "lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
