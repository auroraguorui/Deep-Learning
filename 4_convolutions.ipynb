{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Assignment 4\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.128554\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.637515\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 100: 1.243534\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 150: 0.569015\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 200: 0.818733\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 250: 1.113925\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 300: 0.351469\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 350: 0.436753\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 400: 0.403919\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 450: 0.843620\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 500: 0.691798\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 550: 0.748179\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 600: 0.343421\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 650: 0.885327\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 700: 0.624434\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 750: 0.080481\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 800: 0.633096\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 850: 0.860296\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 900: 0.636232\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 950: 0.600232\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1000: 0.378231\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model_pool(data, train=False):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "        conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    \n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model_pool(tf_train_dataset, True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "   \n",
    "    # adding regularizers\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases)\n",
    "                   )\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 3e-4 * regularizers\n",
    "  \n",
    "    # Optimizer: set up a variable that's incremented once per batch and controls the learning rate decay.\n",
    "    batch = tf.Variable(0)\n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * batch_size,  # Current index into the dataset.\n",
    "        train_labels.shape[0],          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss,global_step=batch)\n",
    "   \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model_pool(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model_pool(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.88005\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 12.5%\n",
      "Minibatch loss at step 50 : 1.57972\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 100 : 1.08866\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 150 : 1.19095\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 200 : 1.10099\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 250 : 0.904118\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 300 : 1.03864\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 350 : 0.882055\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 400 : 0.987214\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 450 : 0.796762\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 500 : 0.679797\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 550 : 0.970096\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 600 : 0.928606\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 650 : 0.787269\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 700 : 0.821542\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 750 : 0.826855\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 800 : 0.59075\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 850 : 0.767693\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 900 : 0.728691\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 950 : 0.895186\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000 : 0.819294\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1050 : 0.565192\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 1100 : 0.793132\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1150 : 0.715296\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1200 : 0.604698\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1250 : 0.542725\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1300 : 0.729431\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1350 : 0.635907\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1400 : 0.597719\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1450 : 0.66225\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1500 : 0.480721\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1550 : 0.676053\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 1600 : 0.69628\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1650 : 0.58438\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1700 : 0.550357\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1750 : 0.602783\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1800 : 0.625531\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1850 : 0.713588\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1900 : 0.782471\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1950 : 0.460377\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2000 : 0.443882\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2050 : 0.579763\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2100 : 0.530514\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2150 : 0.585428\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2200 : 0.61174\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2250 : 0.654727\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2300 : 0.648941\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2350 : 0.481566\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2400 : 0.556875\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2450 : 0.644051\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2500 : 0.544041\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2550 : 0.392376\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2600 : 0.680604\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2650 : 0.442764\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2700 : 0.626532\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2750 : 0.402525\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2800 : 0.428894\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2850 : 0.438793\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2900 : 0.466645\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2950 : 0.517381\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3000 : 0.591681\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.5%\n",
      "Test accuracy: 93.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in xrange(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print(\"Minibatch loss at step\", step, \":\", l)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "batch_size=64\n",
    "PIXEL_DEPTH = 255\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 2.7 ms\n",
      "Minibatch loss: 9.657, learning rate: 0.010000\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 18.9%\n",
      "Step 100 (epoch 0.03), 155.8 ms\n",
      "Minibatch loss: 3.597, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.9%\n",
      "Step 200 (epoch 0.06), 152.9 ms\n",
      "Minibatch loss: 3.588, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.1%\n",
      "Step 300 (epoch 0.10), 156.0 ms\n",
      "Minibatch loss: 3.895, learning rate: 0.010000\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.8%\n",
      "Step 400 (epoch 0.13), 153.0 ms\n",
      "Minibatch loss: 3.810, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.1%\n",
      "Step 500 (epoch 0.16), 155.7 ms\n",
      "Minibatch loss: 3.351, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.5%\n",
      "Step 600 (epoch 0.19), 153.2 ms\n",
      "Minibatch loss: 3.475, learning rate: 0.010000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.1%\n",
      "Step 700 (epoch 0.22), 155.5 ms\n",
      "Minibatch loss: 3.278, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.3%\n",
      "Step 800 (epoch 0.26), 156.4 ms\n",
      "Minibatch loss: 3.387, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Step 900 (epoch 0.29), 155.7 ms\n",
      "Minibatch loss: 3.332, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.7%\n",
      "Step 1000 (epoch 0.32), 156.1 ms\n",
      "Minibatch loss: 3.237, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Step 1100 (epoch 0.35), 155.5 ms\n",
      "Minibatch loss: 3.622, learning rate: 0.010000\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.5%\n",
      "Step 1200 (epoch 0.38), 156.6 ms\n",
      "Minibatch loss: 3.105, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Step 1300 (epoch 0.42), 152.9 ms\n",
      "Minibatch loss: 3.024, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Step 1400 (epoch 0.45), 156.2 ms\n",
      "Minibatch loss: 3.345, learning rate: 0.010000\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1500 (epoch 0.48), 155.5 ms\n",
      "Minibatch loss: 3.129, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1600 (epoch 0.51), 156.5 ms\n",
      "Minibatch loss: 2.886, learning rate: 0.010000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Step 1700 (epoch 0.54), 155.9 ms\n",
      "Minibatch loss: 3.086, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.1%\n",
      "Step 1800 (epoch 0.58), 158.6 ms\n",
      "Minibatch loss: 3.017, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.0%\n",
      "Step 1900 (epoch 0.61), 159.9 ms\n",
      "Minibatch loss: 2.995, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Step 2000 (epoch 0.64), 166.0 ms\n",
      "Minibatch loss: 2.961, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Step 2100 (epoch 0.67), 156.6 ms\n",
      "Minibatch loss: 2.728, learning rate: 0.010000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.5%\n",
      "Step 2200 (epoch 0.70), 155.8 ms\n",
      "Minibatch loss: 2.915, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2300 (epoch 0.74), 153.9 ms\n",
      "Minibatch loss: 3.076, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2400 (epoch 0.77), 156.2 ms\n",
      "Minibatch loss: 2.833, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2500 (epoch 0.80), 156.4 ms\n",
      "Minibatch loss: 2.775, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2600 (epoch 0.83), 156.6 ms\n",
      "Minibatch loss: 2.698, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2700 (epoch 0.86), 156.6 ms\n",
      "Minibatch loss: 2.919, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Step 2800 (epoch 0.90), 156.6 ms\n",
      "Minibatch loss: 2.756, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Step 2900 (epoch 0.93), 156.8 ms\n",
      "Minibatch loss: 2.822, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Step 3000 (epoch 0.96), 156.2 ms\n",
      "Minibatch loss: 2.680, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3100 (epoch 0.99), 157.0 ms\n",
      "Minibatch loss: 2.808, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Step 3200 (epoch 1.02), 156.5 ms\n",
      "Minibatch loss: 2.580, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3300 (epoch 1.06), 156.9 ms\n",
      "Minibatch loss: 2.613, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3400 (epoch 1.09), 156.9 ms\n",
      "Minibatch loss: 2.411, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3500 (epoch 1.12), 156.4 ms\n",
      "Minibatch loss: 2.503, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Step 3600 (epoch 1.15), 156.4 ms\n",
      "Minibatch loss: 2.683, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Step 3700 (epoch 1.18), 156.6 ms\n",
      "Minibatch loss: 2.407, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3800 (epoch 1.22), 153.1 ms\n",
      "Minibatch loss: 2.650, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3900 (epoch 1.25), 153.8 ms\n",
      "Minibatch loss: 2.348, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Step 4000 (epoch 1.28), 156.6 ms\n",
      "Minibatch loss: 2.486, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4100 (epoch 1.31), 152.9 ms\n",
      "Minibatch loss: 2.554, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4200 (epoch 1.34), 156.2 ms\n",
      "Minibatch loss: 2.617, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4300 (epoch 1.38), 156.4 ms\n",
      "Minibatch loss: 2.368, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4400 (epoch 1.41), 157.3 ms\n",
      "Minibatch loss: 2.322, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4500 (epoch 1.44), 156.9 ms\n",
      "Minibatch loss: 2.191, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4600 (epoch 1.47), 156.5 ms\n",
      "Minibatch loss: 2.521, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4700 (epoch 1.50), 153.1 ms\n",
      "Minibatch loss: 2.422, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4800 (epoch 1.54), 157.0 ms\n",
      "Minibatch loss: 2.434, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4900 (epoch 1.57), 155.7 ms\n",
      "Minibatch loss: 2.143, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5000 (epoch 1.60), 156.3 ms\n",
      "Minibatch loss: 2.295, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5100 (epoch 1.63), 156.0 ms\n",
      "Minibatch loss: 2.468, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5200 (epoch 1.66), 153.7 ms\n",
      "Minibatch loss: 2.412, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5300 (epoch 1.70), 156.1 ms\n",
      "Minibatch loss: 2.347, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.2%\n",
      "Step 5400 (epoch 1.73), 157.0 ms\n",
      "Minibatch loss: 2.261, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5500 (epoch 1.76), 156.7 ms\n",
      "Minibatch loss: 2.147, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5600 (epoch 1.79), 154.0 ms\n",
      "Minibatch loss: 2.099, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5700 (epoch 1.82), 153.5 ms\n",
      "Minibatch loss: 2.169, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5800 (epoch 1.86), 156.5 ms\n",
      "Minibatch loss: 2.217, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5900 (epoch 1.89), 153.5 ms\n",
      "Minibatch loss: 2.049, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Step 6000 (epoch 1.92), 155.8 ms\n",
      "Minibatch loss: 1.867, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6100 (epoch 1.95), 156.5 ms\n",
      "Minibatch loss: 2.008, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6200 (epoch 1.98), 156.1 ms\n",
      "Minibatch loss: 2.086, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6300 (epoch 2.02), 156.0 ms\n",
      "Minibatch loss: 2.119, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6400 (epoch 2.05), 156.1 ms\n",
      "Minibatch loss: 2.040, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6500 (epoch 2.08), 156.4 ms\n",
      "Minibatch loss: 1.847, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Step 6600 (epoch 2.11), 153.3 ms\n",
      "Minibatch loss: 1.869, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6700 (epoch 2.14), 156.5 ms\n",
      "Minibatch loss: 2.043, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6800 (epoch 2.18), 156.1 ms\n",
      "Minibatch loss: 1.769, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6900 (epoch 2.21), 153.0 ms\n",
      "Minibatch loss: 1.878, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7000 (epoch 2.24), 156.2 ms\n",
      "Minibatch loss: 1.916, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7100 (epoch 2.27), 157.1 ms\n",
      "Minibatch loss: 1.873, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7200 (epoch 2.30), 156.2 ms\n",
      "Minibatch loss: 1.976, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7300 (epoch 2.34), 153.2 ms\n",
      "Minibatch loss: 1.913, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7400 (epoch 2.37), 155.7 ms\n",
      "Minibatch loss: 2.119, learning rate: 0.009025\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7500 (epoch 2.40), 156.7 ms\n",
      "Minibatch loss: 1.914, learning rate: 0.009025\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Step 7600 (epoch 2.43), 155.9 ms\n",
      "Minibatch loss: 1.893, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7700 (epoch 2.46), 153.1 ms\n",
      "Minibatch loss: 1.796, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7800 (epoch 2.50), 155.6 ms\n",
      "Minibatch loss: 1.747, learning rate: 0.009025\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7900 (epoch 2.53), 156.5 ms\n",
      "Minibatch loss: 1.791, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8000 (epoch 2.56), 156.5 ms\n",
      "Minibatch loss: 1.792, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8100 (epoch 2.59), 156.2 ms\n",
      "Minibatch loss: 1.790, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8200 (epoch 2.62), 155.8 ms\n",
      "Minibatch loss: 1.657, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8300 (epoch 2.66), 156.1 ms\n",
      "Minibatch loss: 1.676, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8400 (epoch 2.69), 156.4 ms\n",
      "Minibatch loss: 1.897, learning rate: 0.009025\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.0%\n",
      "Step 8500 (epoch 2.72), 156.6 ms\n",
      "Minibatch loss: 1.910, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8600 (epoch 2.75), 156.5 ms\n",
      "Minibatch loss: 1.678, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8700 (epoch 2.78), 156.3 ms\n",
      "Minibatch loss: 1.836, learning rate: 0.009025\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8800 (epoch 2.82), 156.9 ms\n",
      "Minibatch loss: 1.745, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8900 (epoch 2.85), 156.7 ms\n",
      "Minibatch loss: 1.637, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.6%\n",
      "Step 9000 (epoch 2.88), 153.1 ms\n",
      "Minibatch loss: 1.531, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Step 9100 (epoch 2.91), 156.7 ms\n",
      "Minibatch loss: 1.702, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9200 (epoch 2.94), 156.4 ms\n",
      "Minibatch loss: 1.647, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Step 9300 (epoch 2.98), 157.0 ms\n",
      "Minibatch loss: 1.657, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.0%\n",
      "Step 9400 (epoch 3.01), 156.8 ms\n",
      "Minibatch loss: 1.592, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Step 9500 (epoch 3.04), 156.3 ms\n",
      "Minibatch loss: 1.633, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9600 (epoch 3.07), 156.5 ms\n",
      "Minibatch loss: 1.509, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 9700 (epoch 3.10), 156.7 ms\n",
      "Minibatch loss: 1.420, learning rate: 0.008574\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.1%\n",
      "Step 9800 (epoch 3.14), 157.6 ms\n",
      "Minibatch loss: 1.559, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Step 9900 (epoch 3.17), 156.8 ms\n",
      "Minibatch loss: 1.616, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10000 (epoch 3.20), 156.7 ms\n",
      "Minibatch loss: 1.403, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Step 10100 (epoch 3.23), 161.6 ms\n",
      "Minibatch loss: 1.537, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 10200 (epoch 3.26), 168.1 ms\n",
      "Minibatch loss: 1.379, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10300 (epoch 3.30), 164.3 ms\n",
      "Minibatch loss: 1.501, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10400 (epoch 3.33), 160.8 ms\n",
      "Minibatch loss: 1.476, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10500 (epoch 3.36), 157.7 ms\n",
      "Minibatch loss: 1.563, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10600 (epoch 3.39), 156.1 ms\n",
      "Minibatch loss: 1.323, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 10700 (epoch 3.42), 156.2 ms\n",
      "Minibatch loss: 1.448, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10800 (epoch 3.46), 156.1 ms\n",
      "Minibatch loss: 1.695, learning rate: 0.008574\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10900 (epoch 3.49), 153.0 ms\n",
      "Minibatch loss: 1.440, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11000 (epoch 3.52), 155.9 ms\n",
      "Minibatch loss: 1.389, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11100 (epoch 3.55), 152.5 ms\n",
      "Minibatch loss: 1.345, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11200 (epoch 3.58), 156.4 ms\n",
      "Minibatch loss: 1.373, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11300 (epoch 3.62), 155.2 ms\n",
      "Minibatch loss: 1.404, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11400 (epoch 3.65), 156.4 ms\n",
      "Minibatch loss: 1.291, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11500 (epoch 3.68), 155.9 ms\n",
      "Minibatch loss: 1.419, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11600 (epoch 3.71), 156.5 ms\n",
      "Minibatch loss: 1.585, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.5%\n",
      "Step 11700 (epoch 3.74), 158.1 ms\n",
      "Minibatch loss: 1.362, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11800 (epoch 3.78), 171.7 ms\n",
      "Minibatch loss: 1.304, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 11900 (epoch 3.81), 163.0 ms\n",
      "Minibatch loss: 1.347, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12000 (epoch 3.84), 155.9 ms\n",
      "Minibatch loss: 1.357, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12100 (epoch 3.87), 153.2 ms\n",
      "Minibatch loss: 1.188, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.6%\n",
      "Step 12200 (epoch 3.90), 155.7 ms\n",
      "Minibatch loss: 1.281, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12300 (epoch 3.94), 155.9 ms\n",
      "Minibatch loss: 1.195, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 12400 (epoch 3.97), 156.1 ms\n",
      "Minibatch loss: 1.327, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 12500 (epoch 4.00), 156.4 ms\n",
      "Minibatch loss: 1.146, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12600 (epoch 4.03), 156.4 ms\n",
      "Minibatch loss: 1.368, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12700 (epoch 4.06), 153.5 ms\n",
      "Minibatch loss: 1.216, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12800 (epoch 4.10), 155.6 ms\n",
      "Minibatch loss: 1.289, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 12900 (epoch 4.13), 156.3 ms\n",
      "Minibatch loss: 1.083, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13000 (epoch 4.16), 152.5 ms\n",
      "Minibatch loss: 1.406, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13100 (epoch 4.19), 153.5 ms\n",
      "Minibatch loss: 1.265, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13200 (epoch 4.22), 156.2 ms\n",
      "Minibatch loss: 1.309, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13300 (epoch 4.26), 156.7 ms\n",
      "Minibatch loss: 1.069, learning rate: 0.008145\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13400 (epoch 4.29), 156.2 ms\n",
      "Minibatch loss: 1.054, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13500 (epoch 4.32), 156.4 ms\n",
      "Minibatch loss: 1.066, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13600 (epoch 4.35), 155.7 ms\n",
      "Minibatch loss: 1.272, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13700 (epoch 4.38), 152.9 ms\n",
      "Minibatch loss: 1.230, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13800 (epoch 4.42), 155.6 ms\n",
      "Minibatch loss: 1.132, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.5%\n",
      "Step 13900 (epoch 4.45), 155.8 ms\n",
      "Minibatch loss: 1.171, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14000 (epoch 4.48), 155.6 ms\n",
      "Minibatch loss: 1.032, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14100 (epoch 4.51), 156.4 ms\n",
      "Minibatch loss: 1.121, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14200 (epoch 4.54), 156.1 ms\n",
      "Minibatch loss: 1.177, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14300 (epoch 4.58), 153.4 ms\n",
      "Minibatch loss: 1.136, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14400 (epoch 4.61), 155.5 ms\n",
      "Minibatch loss: 1.021, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14500 (epoch 4.64), 156.0 ms\n",
      "Minibatch loss: 1.102, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14600 (epoch 4.67), 155.6 ms\n",
      "Minibatch loss: 1.228, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14700 (epoch 4.70), 156.5 ms\n",
      "Minibatch loss: 0.938, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.6%\n",
      "Step 14800 (epoch 4.74), 156.4 ms\n",
      "Minibatch loss: 1.274, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 14900 (epoch 4.77), 156.3 ms\n",
      "Minibatch loss: 1.055, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15000 (epoch 4.80), 156.1 ms\n",
      "Minibatch loss: 1.039, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15100 (epoch 4.83), 155.9 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15200 (epoch 4.86), 156.2 ms\n",
      "Minibatch loss: 1.085, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15300 (epoch 4.90), 152.3 ms\n",
      "Minibatch loss: 1.178, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15400 (epoch 4.93), 156.6 ms\n",
      "Minibatch loss: 1.040, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15500 (epoch 4.96), 155.7 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 15600 (epoch 4.99), 156.0 ms\n",
      "Minibatch loss: 1.044, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 15700 (epoch 5.02), 152.0 ms\n",
      "Minibatch loss: 1.144, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15800 (epoch 5.06), 155.8 ms\n",
      "Minibatch loss: 1.062, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15900 (epoch 5.09), 155.6 ms\n",
      "Minibatch loss: 0.887, learning rate: 0.007738\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16000 (epoch 5.12), 155.8 ms\n",
      "Minibatch loss: 1.105, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16100 (epoch 5.15), 155.9 ms\n",
      "Minibatch loss: 1.040, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16200 (epoch 5.18), 156.7 ms\n",
      "Minibatch loss: 0.931, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16300 (epoch 5.22), 155.5 ms\n",
      "Minibatch loss: 1.002, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16400 (epoch 5.25), 156.1 ms\n",
      "Minibatch loss: 1.018, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 16500 (epoch 5.28), 155.2 ms\n",
      "Minibatch loss: 0.970, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 16600 (epoch 5.31), 155.8 ms\n",
      "Minibatch loss: 1.056, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16700 (epoch 5.34), 154.8 ms\n",
      "Minibatch loss: 1.120, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16800 (epoch 5.38), 155.4 ms\n",
      "Minibatch loss: 1.022, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16900 (epoch 5.41), 155.1 ms\n",
      "Minibatch loss: 1.077, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17000 (epoch 5.44), 156.0 ms\n",
      "Minibatch loss: 0.997, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17100 (epoch 5.47), 155.8 ms\n",
      "Minibatch loss: 1.079, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17200 (epoch 5.50), 155.7 ms\n",
      "Minibatch loss: 0.862, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 17300 (epoch 5.54), 156.2 ms\n",
      "Minibatch loss: 0.964, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17400 (epoch 5.57), 152.3 ms\n",
      "Minibatch loss: 0.929, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17500 (epoch 5.60), 156.0 ms\n",
      "Minibatch loss: 0.893, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17600 (epoch 5.63), 155.2 ms\n",
      "Minibatch loss: 0.961, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17700 (epoch 5.66), 152.0 ms\n",
      "Minibatch loss: 0.876, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17800 (epoch 5.70), 156.1 ms\n",
      "Minibatch loss: 0.927, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 17900 (epoch 5.73), 156.4 ms\n",
      "Minibatch loss: 0.924, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18000 (epoch 5.76), 155.0 ms\n",
      "Minibatch loss: 0.974, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18100 (epoch 5.79), 156.2 ms\n",
      "Minibatch loss: 0.813, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18200 (epoch 5.82), 155.5 ms\n",
      "Minibatch loss: 0.873, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18300 (epoch 5.86), 156.4 ms\n",
      "Minibatch loss: 0.880, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18400 (epoch 5.89), 155.8 ms\n",
      "Minibatch loss: 0.898, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18500 (epoch 5.92), 156.3 ms\n",
      "Minibatch loss: 0.853, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18600 (epoch 5.95), 152.7 ms\n",
      "Minibatch loss: 0.826, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 18700 (epoch 5.98), 155.8 ms\n",
      "Minibatch loss: 0.776, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 18800 (epoch 6.02), 155.4 ms\n",
      "Minibatch loss: 0.888, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 18900 (epoch 6.05), 156.5 ms\n",
      "Minibatch loss: 0.909, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19000 (epoch 6.08), 156.0 ms\n",
      "Minibatch loss: 0.814, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 19100 (epoch 6.11), 156.3 ms\n",
      "Minibatch loss: 0.967, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19200 (epoch 6.14), 156.0 ms\n",
      "Minibatch loss: 0.739, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19300 (epoch 6.18), 155.8 ms\n",
      "Minibatch loss: 0.853, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19400 (epoch 6.21), 152.2 ms\n",
      "Minibatch loss: 0.748, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19500 (epoch 6.24), 152.6 ms\n",
      "Minibatch loss: 0.821, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 19600 (epoch 6.27), 152.4 ms\n",
      "Minibatch loss: 0.744, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19700 (epoch 6.30), 152.7 ms\n",
      "Minibatch loss: 0.918, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19800 (epoch 6.34), 162.5 ms\n",
      "Minibatch loss: 0.744, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19900 (epoch 6.37), 162.6 ms\n",
      "Minibatch loss: 0.771, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 20000 (epoch 6.40), 155.6 ms\n",
      "Minibatch loss: 0.707, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20100 (epoch 6.43), 155.5 ms\n",
      "Minibatch loss: 1.147, learning rate: 0.007351\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20200 (epoch 6.46), 152.6 ms\n",
      "Minibatch loss: 0.890, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20300 (epoch 6.50), 155.2 ms\n",
      "Minibatch loss: 0.882, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20400 (epoch 6.53), 155.9 ms\n",
      "Minibatch loss: 1.009, learning rate: 0.007351\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20500 (epoch 6.56), 155.6 ms\n",
      "Minibatch loss: 0.846, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 20600 (epoch 6.59), 152.2 ms\n",
      "Minibatch loss: 0.702, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20700 (epoch 6.62), 155.0 ms\n",
      "Minibatch loss: 0.623, learning rate: 0.007351\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20800 (epoch 6.66), 156.0 ms\n",
      "Minibatch loss: 0.839, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20900 (epoch 6.69), 155.5 ms\n",
      "Minibatch loss: 0.794, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21000 (epoch 6.72), 155.9 ms\n",
      "Minibatch loss: 0.812, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21100 (epoch 6.75), 155.5 ms\n",
      "Minibatch loss: 0.847, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21200 (epoch 6.78), 155.6 ms\n",
      "Minibatch loss: 0.684, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21300 (epoch 6.82), 151.9 ms\n",
      "Minibatch loss: 0.693, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21400 (epoch 6.85), 156.0 ms\n",
      "Minibatch loss: 0.749, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21500 (epoch 6.88), 155.3 ms\n",
      "Minibatch loss: 0.816, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21600 (epoch 6.91), 155.8 ms\n",
      "Minibatch loss: 0.697, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21700 (epoch 6.94), 155.1 ms\n",
      "Minibatch loss: 0.728, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21800 (epoch 6.98), 155.7 ms\n",
      "Minibatch loss: 0.796, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21900 (epoch 7.01), 155.2 ms\n",
      "Minibatch loss: 0.768, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22000 (epoch 7.04), 152.6 ms\n",
      "Minibatch loss: 0.609, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22100 (epoch 7.07), 155.3 ms\n",
      "Minibatch loss: 0.786, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22200 (epoch 7.10), 156.0 ms\n",
      "Minibatch loss: 0.802, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 22300 (epoch 7.14), 155.3 ms\n",
      "Minibatch loss: 0.619, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 22400 (epoch 7.17), 155.9 ms\n",
      "Minibatch loss: 0.910, learning rate: 0.006983\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22500 (epoch 7.20), 156.1 ms\n",
      "Minibatch loss: 0.718, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22600 (epoch 7.23), 155.6 ms\n",
      "Minibatch loss: 0.807, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22700 (epoch 7.26), 155.2 ms\n",
      "Minibatch loss: 0.872, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22800 (epoch 7.30), 156.0 ms\n",
      "Minibatch loss: 0.689, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22900 (epoch 7.33), 155.5 ms\n",
      "Minibatch loss: 0.755, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23000 (epoch 7.36), 152.2 ms\n",
      "Minibatch loss: 0.772, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 23100 (epoch 7.39), 156.0 ms\n",
      "Minibatch loss: 0.627, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23200 (epoch 7.42), 155.7 ms\n",
      "Minibatch loss: 1.055, learning rate: 0.006983\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 91.9%\n",
      "Step 23300 (epoch 7.46), 155.4 ms\n",
      "Minibatch loss: 0.715, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 23400 (epoch 7.49), 152.1 ms\n",
      "Minibatch loss: 0.641, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23500 (epoch 7.52), 156.2 ms\n",
      "Minibatch loss: 0.788, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23600 (epoch 7.55), 155.5 ms\n",
      "Minibatch loss: 0.589, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23700 (epoch 7.58), 152.3 ms\n",
      "Minibatch loss: 0.527, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23800 (epoch 7.62), 152.6 ms\n",
      "Minibatch loss: 0.646, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 23900 (epoch 7.65), 156.2 ms\n",
      "Minibatch loss: 0.632, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24000 (epoch 7.68), 155.6 ms\n",
      "Minibatch loss: 0.834, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.1%\n",
      "Step 24100 (epoch 7.71), 152.3 ms\n",
      "Minibatch loss: 0.568, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24200 (epoch 7.74), 155.7 ms\n",
      "Minibatch loss: 0.640, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24300 (epoch 7.78), 155.8 ms\n",
      "Minibatch loss: 0.695, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24400 (epoch 7.81), 155.9 ms\n",
      "Minibatch loss: 0.624, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24500 (epoch 7.84), 156.1 ms\n",
      "Minibatch loss: 0.974, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24600 (epoch 7.87), 155.3 ms\n",
      "Minibatch loss: 0.651, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24700 (epoch 7.90), 155.5 ms\n",
      "Minibatch loss: 0.501, learning rate: 0.006983\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24800 (epoch 7.94), 152.1 ms\n",
      "Minibatch loss: 0.764, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24900 (epoch 7.97), 152.8 ms\n",
      "Minibatch loss: 0.593, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25000 (epoch 8.00), 156.0 ms\n",
      "Minibatch loss: 0.726, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25100 (epoch 8.03), 155.8 ms\n",
      "Minibatch loss: 0.829, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25200 (epoch 8.06), 155.9 ms\n",
      "Minibatch loss: 0.592, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25300 (epoch 8.10), 156.3 ms\n",
      "Minibatch loss: 0.600, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25400 (epoch 8.13), 155.7 ms\n",
      "Minibatch loss: 0.763, learning rate: 0.006634\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25500 (epoch 8.16), 155.7 ms\n",
      "Minibatch loss: 0.583, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25600 (epoch 8.19), 152.2 ms\n",
      "Minibatch loss: 0.646, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25700 (epoch 8.22), 155.8 ms\n",
      "Minibatch loss: 0.556, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25800 (epoch 8.26), 155.3 ms\n",
      "Minibatch loss: 0.614, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25900 (epoch 8.29), 155.7 ms\n",
      "Minibatch loss: 0.775, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26000 (epoch 8.32), 152.4 ms\n",
      "Minibatch loss: 0.567, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26100 (epoch 8.35), 152.7 ms\n",
      "Minibatch loss: 0.577, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26200 (epoch 8.38), 155.8 ms\n",
      "Minibatch loss: 0.603, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26300 (epoch 8.42), 155.2 ms\n",
      "Minibatch loss: 0.624, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26400 (epoch 8.45), 152.4 ms\n",
      "Minibatch loss: 0.639, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26500 (epoch 8.48), 155.2 ms\n",
      "Minibatch loss: 0.682, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26600 (epoch 8.51), 156.2 ms\n",
      "Minibatch loss: 0.598, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Step 26700 (epoch 8.54), 156.2 ms\n",
      "Minibatch loss: 0.526, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26800 (epoch 8.58), 155.6 ms\n",
      "Minibatch loss: 0.750, learning rate: 0.006634\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 26900 (epoch 8.61), 155.7 ms\n",
      "Minibatch loss: 0.664, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27000 (epoch 8.64), 155.9 ms\n",
      "Minibatch loss: 0.753, learning rate: 0.006634\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27100 (epoch 8.67), 155.9 ms\n",
      "Minibatch loss: 0.540, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27200 (epoch 8.70), 155.9 ms\n",
      "Minibatch loss: 0.663, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.6%\n",
      "Step 27300 (epoch 8.74), 156.1 ms\n",
      "Minibatch loss: 0.472, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27400 (epoch 8.77), 156.1 ms\n",
      "Minibatch loss: 0.583, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27500 (epoch 8.80), 155.8 ms\n",
      "Minibatch loss: 0.658, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27600 (epoch 8.83), 155.9 ms\n",
      "Minibatch loss: 0.487, learning rate: 0.006634\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27700 (epoch 8.86), 155.8 ms\n",
      "Minibatch loss: 0.636, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27800 (epoch 8.90), 162.7 ms\n",
      "Minibatch loss: 0.497, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27900 (epoch 8.93), 159.8 ms\n",
      "Minibatch loss: 0.708, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28000 (epoch 8.96), 156.2 ms\n",
      "Minibatch loss: 0.597, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28100 (epoch 8.99), 156.7 ms\n",
      "Minibatch loss: 0.645, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28200 (epoch 9.02), 152.9 ms\n",
      "Minibatch loss: 0.622, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Step 28300 (epoch 9.06), 155.8 ms\n",
      "Minibatch loss: 0.527, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28400 (epoch 9.09), 155.9 ms\n",
      "Minibatch loss: 0.489, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28500 (epoch 9.12), 155.7 ms\n",
      "Minibatch loss: 0.552, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28600 (epoch 9.15), 155.8 ms\n",
      "Minibatch loss: 0.541, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28700 (epoch 9.18), 156.2 ms\n",
      "Minibatch loss: 0.606, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28800 (epoch 9.22), 155.7 ms\n",
      "Minibatch loss: 0.481, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28900 (epoch 9.25), 151.8 ms\n",
      "Minibatch loss: 0.570, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29000 (epoch 9.28), 155.4 ms\n",
      "Minibatch loss: 0.449, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29100 (epoch 9.31), 155.5 ms\n",
      "Minibatch loss: 0.498, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29200 (epoch 9.34), 156.3 ms\n",
      "Minibatch loss: 0.434, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29300 (epoch 9.38), 156.0 ms\n",
      "Minibatch loss: 0.414, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29400 (epoch 9.41), 155.9 ms\n",
      "Minibatch loss: 0.480, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29500 (epoch 9.44), 156.1 ms\n",
      "Minibatch loss: 0.547, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29600 (epoch 9.47), 152.2 ms\n",
      "Minibatch loss: 0.530, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29700 (epoch 9.50), 156.6 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006302\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29800 (epoch 9.54), 156.1 ms\n",
      "Minibatch loss: 0.547, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29900 (epoch 9.57), 156.2 ms\n",
      "Minibatch loss: 0.593, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30000 (epoch 9.60), 155.5 ms\n",
      "Minibatch loss: 0.539, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30100 (epoch 9.63), 156.5 ms\n",
      "Minibatch loss: 0.508, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30200 (epoch 9.66), 155.5 ms\n",
      "Minibatch loss: 0.543, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30300 (epoch 9.70), 156.0 ms\n",
      "Minibatch loss: 0.503, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30400 (epoch 9.73), 152.1 ms\n",
      "Minibatch loss: 0.561, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30500 (epoch 9.76), 152.8 ms\n",
      "Minibatch loss: 0.730, learning rate: 0.006302\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.3%\n",
      "Step 30600 (epoch 9.79), 155.6 ms\n",
      "Minibatch loss: 0.440, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30700 (epoch 9.82), 156.4 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 30800 (epoch 9.86), 155.4 ms\n",
      "Minibatch loss: 0.439, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30900 (epoch 9.89), 155.4 ms\n",
      "Minibatch loss: 0.529, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31000 (epoch 9.92), 156.1 ms\n",
      "Minibatch loss: 0.601, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31100 (epoch 9.95), 155.4 ms\n",
      "Minibatch loss: 0.810, learning rate: 0.006302\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31200 (epoch 9.98), 152.1 ms\n",
      "Minibatch loss: 0.470, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Test accuracy: 97.2%\n"
     ]
    }
   ],
   "source": [
    "def lenet(argv=None):  # pylint: disable=unused-argument\n",
    "  train_size = train_labels.shape[0]\n",
    "\n",
    "  # This is where training samples and labels are fed to the graph.\n",
    "  # These placeholder nodes will be fed a batch of training data at each\n",
    "  # training step using the {feed_dict} argument to the Run() call below.\n",
    "  train_data_node = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(batch_size, image_size, image_size, num_channels))\n",
    "  train_labels_node = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_labels))\n",
    "  eval_data = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(EVAL_BATCH_SIZE, image_size, image_size, num_channels))\n",
    "\n",
    "  # The variables below hold all the trainable weights. They are passed an\n",
    "  # initial value which will be assigned when when we call:\n",
    "  # {tf.initialize_all_variables().run()}\n",
    "  conv1_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, num_channels, 32],  # 5x5 filter, depth 32.\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "  conv2_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, 32, 64],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "      tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * 64, 512],\n",
    "          stddev=0.1,\n",
    "          seed=SEED))\n",
    "  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "  fc2_weights = tf.Variable(\n",
    "      tf.truncated_normal([512, num_labels],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "\n",
    "  # We will replicate the model structure for the training subgraph, as well\n",
    "  # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "  def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "  # Training computation: logits + cross-entropy loss.\n",
    "  logits = model(train_data_node, True)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      logits, train_labels_node))\n",
    "\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "\n",
    "  # Optimizer: set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  batch = tf.Variable(0)\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * batch_size,  # Current index into the dataset.\n",
    "      train_size,          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                         0.9).minimize(loss,\n",
    "                                                       global_step=batch)\n",
    "\n",
    "  # Predictions for the current training minibatch.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Predictions for the test and validation, which we'll compute less often.\n",
    "  eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "  # {eval_data} and pulling the results from {eval_predictions}.\n",
    "  # Saves memory and enables this to run on smaller GPUs.\n",
    "  def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, num_labels), dtype=np.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "  # Create a local session to run the training.\n",
    "  start_time = time.time()\n",
    "  with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    for step in xrange(int(NUM_EPOCHS * train_size) // batch_size):\n",
    "      # Compute the offset of the current minibatch in the data.\n",
    "      # Note that we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_size - batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), ...]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "      # This dictionary maps the batch data (as a numpy array) to the\n",
    "      # node in the graph is should be fed to.\n",
    "      feed_dict = {train_data_node: batch_data,\n",
    "                   train_labels_node: batch_labels}\n",
    "      # Run the graph and fetch some of the nodes.\n",
    "      _, l, lr, predictions = sess.run(\n",
    "          [optimizer, loss, learning_rate, train_prediction],\n",
    "          feed_dict=feed_dict)\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "              (step, float(step) * batch_size / train_size,\n",
    "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            eval_in_batches(valid_dataset, sess), valid_labels))\n",
    "        sys.stdout.flush()\n",
    "    # Finally print the result!\n",
    "    test_error = accuracy(eval_in_batches(test_dataset, sess), test_labels)\n",
    "    print('Test accuracy: %.1f%%' % test_error)\n",
    "    \n",
    "lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
